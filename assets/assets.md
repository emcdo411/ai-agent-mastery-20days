# ðŸŽ¨ W4D28 â€” Mastery Program Assets

![Status](https://img.shields.io/badge/Day-28-blueviolet?style=for-the-badge)
![Flowise](https://img.shields.io/badge/Flowise-Agent_Builder-green?style=for-the-badge)
![Ollama](https://img.shields.io/badge/Ollama-Local_LLM-orange?style=for-the-badge)
![Chroma](https://img.shields.io/badge/Chroma-Vector_Store-teal?style=for-the-badge)

---

## ðŸ“‘ Table of Contents
- [ðŸŽ¯ Purpose](#-purpose)
- [ðŸ“‚ Core Assets](#-core-assets)
- [ðŸ“¦ Deliverables](#-deliverables)
- [ðŸ—º Workflow](#-workflow)

---

## ðŸŽ¯ Purpose
This folder holds **Day 28 mastery program assets** â€” foundational files youâ€™ll reference for:
- Explaining **Generative AI vs Agentic AI**  
- Breaking down **Retrieval Augmented Generation (RAG)**  
- Providing visuals, badges, and workflows for presentations or repos  

---

## ðŸ“‚ Core Assets
- `W4D28_generative_vs_agentic.md` â†’ Deep dive on AI modes  
- `W4D28_rag_deepdive.md` â†’ What RAG is + why it matters  
- `W4D28_assets.md` â†’ This asset index file  

---

## ðŸ“¦ Deliverables
- Shields.io badges for repo branding  
- Clickable ToC for quick navigation  
- Mermaid workflow diagrams (dark theme)  

---

## ðŸ—º Workflow

```mermaid
%%{ init: { 'theme': 'dark' } }%%
flowchart LR
    A["ðŸ—‚ Assets (Day28)"]
    B["ðŸ§  Generative vs Agentic AI"]
    C["ðŸ“š RAG Deep Dive"]
    D["ðŸŽ¨ Badges + Visuals"]

    A --> B
    A --> C
    A --> D
````

---

````

---

# ðŸ“‚ File 2: `W4D28_generative_vs_agentic.md`

```markdown
# ðŸ¤– Generative AI vs Agentic AI

![Topic](https://img.shields.io/badge/Deep_Dive-Generative_vs_Agentic-purple?style=flat-square)

---

## ðŸ“‘ Table of Contents
- [âš¡ Quick Summary](#-quick-summary)
- [ðŸ§  Generative AI](#-generative-ai)
- [ðŸ•¹ Agentic AI](#-agentic-ai)
- [âš” Key Differences](#-key-differences)
- [ðŸ—º Workflow](#-workflow)

---

## âš¡ Quick Summary
- **Generative AI**: Models that *generate outputs* (text, image, code).  
- **Agentic AI**: Systems that *act with autonomy*, chaining reasoning, tools, memory, and goals.  

---

## ðŸ§  Generative AI
- Focus: **output generation** (text, image, video, code).  
- Example: GPT writing an essay or Stable Diffusion creating an image.  
- Limitation: **static** â€” responds only to a prompt; no memory or tool use.  

---

## ðŸ•¹ Agentic AI
- Focus: **autonomous actions** with context + tools.  
- Example: Flowise agent using Ollama to fetch repo data, summarize, then output next steps.  
- Strength: **reasoning + planning + acting** over time.  
- Limitation: More complex setup; requires orchestration (memory, retrievers, routers).  

---

## âš” Key Differences

| Feature          | Generative AI             | Agentic AI                    |
|------------------|---------------------------|--------------------------------|
| Output           | Text, code, images        | Actions, plans, multi-step     |
| Context Handling | Single prompt             | Memory + tools + environment   |
| Example          | ChatGPT writing text      | AutoGPT/Flowise RAG pipeline   |
| Dependency       | Model alone               | Model + architecture           |

---

## ðŸ—º Workflow

```mermaid
flowchart TD
    G["ðŸ§  Generative AI"] --> O["Outputs (text, image, code)"]
    A["ðŸ•¹ Agentic AI"] --> P["Plans (multi-step)"]
    A --> T["Uses Tools (DBs, APIs)"]
    A --> M["Keeps Memory"]
````

---

````

---

# ðŸ“‚ File 3: `W4D28_rag_deepdive.md`

```markdown
# ðŸ“š Retrieval Augmented Generation (RAG)

![Topic](https://img.shields.io/badge/Deep_Dive-RAG-blue?style=flat-square)

---

## ðŸ“‘ Table of Contents
- [âš¡ TL;DR](#-tldr)
- [ðŸ” What RAG Is](#-what-rag-is)
- [ðŸ“ˆ Why RAG Matters](#-why-rag-matters)
- [ðŸ›  How RAG Works](#-how-rag-works)
- [ðŸ—º Workflow](#-workflow)

---

## âš¡ TL;DR
RAG = **Retriever + Generator**  
- It *fetches context* from a knowledge base (docs, CSVs, databases)  
- Then passes it into a generative model (LLM)  
- Output = more accurate, grounded answers  

---

## ðŸ” What RAG Is
- **Retrieval**: Grabs most relevant chunks from a vector store (e.g., Chroma).  
- **Augmentation**: Adds those chunks into the modelâ€™s prompt.  
- **Generation**: LLM uses that context to answer grounded in *real data*.  

---

## ðŸ“ˆ Why RAG Matters
- Prevents hallucinations  
- Makes AI **explainable + auditable**  
- Bridges **static LLM knowledge** with **dynamic external data**  
- Critical for **local-first, privacy-safe AI workflows**  

---

## ðŸ›  How RAG Works

1. ðŸ§© **Text Splitter** breaks docs into chunks  
2. ðŸ§¬ **Embeddings** turn chunks â†’ vectors  
3. ðŸ“‚ **Vector Store** indexes & retrieves relevant chunks  
4. ðŸ§  **LLM** generates answer using both **prompt + retrieved context**  

---

## ðŸ—º Workflow

```mermaid
%%{ init: { 'theme': 'dark' } }%%
flowchart LR
    D["ðŸ“‚ Documents"]
    S["âœ‚ï¸ Splitter"]
    E["ðŸ§¬ Embeddings"]
    V["ðŸ—ƒ Vector Store"]
    R["ðŸ”Ž Retriever"]
    L["ðŸ§  LLM"]

    D --> S --> E --> V
    V --> R --> L
````

---

```

---

âœ… Now youâ€™ll have:
- `W4D28_assets.md` â†’ **index** of Day 28 deliverables  
- `W4D28_generative_vs_agentic.md` â†’ **deep dive** with differences + workflow  
- `W4D28_rag_deepdive.md` â†’ **deep dive** on RAG  

---
