# ğŸš€ Day 22 â€” FlowiseAI Starter: Local Strategic Agent (Ollama + Chroma RAG)

## ğŸ¯ Objective

Spin up a **100% free + local AI agent** in FlowiseAI that:

* Uses **Ollama** (`llama3.1:8b` or `phi3:mini`) as your LLM
* Indexes `.md` and `.csv` files with **Chroma vector search**
* Runs in **Flowise** (no-code UI) on `http://localhost:3000`
* Outputs filename-cited responses you can **commit to GitHub**

â± Target Vibe: **â‰¤ 30 minutes**

---

## âš¡ï¸ Quick Setup (for the impatient)

1. **Install Ollama**

   ```powershell
   winget install Ollama.Ollama
   ollama pull phi3:mini
   ollama pull nomic-embed-text
   ```

2. **Run Flowise (Docker)**

   ```bash
   docker run -d -p 3000:3000 -v flowise_data:/root/.flowise flowiseai/flowise
   ```

3. **Build Chatflow (Flowise UI)**

   * Add nodes: **Chat Input â†’ Local Files â†’ Text Splitter â†’ Ollama Embeddings â†’ Chroma â†’ Retriever â†’ Prompt â†’ LLM â†’ Chat Output**
   * System Prompt:

     ```
     You are a Strategic AI Coach. Use ONLY retrieved repo context. 
     Cite filenames. Output bullet points + an action list.
     ```

4. **Test Prompt**

   > â€œSummarize all deliverables due in Week 2 and cite the filenames.â€

5. **Export**

   * Save as: `W4D22_flowise_chatflow.json`
   * Commit screenshot: `W4D22_flowise_screenshot.png`

Done âœ… â†’ Youâ€™ve got a local RAG agent with filename citations.

---

## ğŸ”¬ Deep Dive (for vibe coders who want the why)

### 1ï¸âƒ£ Install Ollama (Local LLM)

* Windows: `winget install Ollama.Ollama`
* macOS/Linux: [ollama.com/download](https://ollama.com/download)

**Pull models**:

* Heavy-duty: `ollama pull llama3.1:8b`
* Lightweight: `ollama pull phi3:mini`
* Embeddings: `ollama pull nomic-embed-text`

---

### 2ï¸âƒ£ Run Flowise

* **Docker** (best):

  ```bash
  docker run -d -p 3000:3000 -v flowise_data:/root/.flowise flowiseai/flowise
  ```
* **Node.js**:

  ```bash
  npx flowise start
  ```

---

### 3ï¸âƒ£ Build the RAG Chatflow

**Nodes to add (drag in order):**

1. Chat Input
2. Document Loader â†’ Local Files (`*.md, *.csv`)
3. Text Splitter (chunk 1000, overlap 150)
4. Embeddings â†’ Ollama (`nomic-embed-text`)
5. Vector Store â†’ Chroma (`aimastery_w4`)
6. Retriever
7. Prompt Template (strategic system role)
8. LLM â†’ Ollama (phi3 or llama3.1)
9. Chat Memory (optional)
10. Chat Output

**Wiring:**

```
Input â†’ Loader â†’ Splitter â†’ Embeddings â†’ Chroma â†’ Retriever â†’ Prompt â†’ LLM â†’ Output
```

---

### 4ï¸âƒ£ Test Prompts

* â€œFrom this repo, what are the deliverables due in Week 2?â€
* â€œGive me a prep checklist for Day 21 with file references.â€
* â€œSummarize Week 1 outcomes for an MBA vs a veteran.â€

---

### 5ï¸âƒ£ Deliverables

* `W4D22_flowise_chatflow.json` (exported flow)
* `W4D22_flowise_notes.md` (1â€“2 paragraphs: model, collection, files indexed, sample Q\&A)
* `W4D22_flowise_screenshot.png` (optional UI snap)

---

## ğŸ§  Troubleshooting

* Ollama not found? â†’ restart service on `http://localhost:11434`
* Flowise blank screen? â†’ `docker logs flowise` for errors
* Too slow? â†’ switch to `phi3:mini` and lower Retriever Top-K

---

## ğŸ¯ Role Relevance

* **Analysts / MBA / PMP** â€” filename-cited briefs
* **Entrepreneurs** â€” private doc Q\&A (no cloud APIs)
* **Military Transitioners** â€” structured SITREP agent with task focus

---

âœ¨ *Day 22 vibe: youâ€™ve now got your own **private AI analyst** that reads your repo, cites filenames, and runs 100% free & offline.*

---

Would you like me to also create a **W4D22\_flowise\_notes.md** template (with placeholders for model, collection, files, and a sample Q\&A) so your repo has the full trio (chatflow + screenshot + notes)?

