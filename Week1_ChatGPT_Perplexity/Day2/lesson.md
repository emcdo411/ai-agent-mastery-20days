# Week 1 â€” Day 2: Structured Prompt Engineering (RTF + PICO with ChatGPT-5)

**Save as:** `week1/day2_structured_prompt_engineering.md`

---

## ğŸ¯ Purpose

Introduce reproducible, structured prompt-engineering techniques that map directly to model evaluation and fine-tuning workflows.
By Day 2, learners will understand how to design **prompt contracts**â€”inputs and expected outputs that can later feed into LLM evaluation datasets or fine-tuning scripts.

---

## ğŸ§© Learning Objectives

1. Apply the **RTF** (Role â†’ Task â†’ Format) and **PICO** (Persona Â· Instructions Â· Context Â· Output) frameworks.
2. Build a reusable prompt template that drives consistent, testable results.
3. Evaluate and log differences in reasoning between **ChatGPT-5** and **Perplexity AI**.
4. Save outputs in repo-friendly formats (`.txt`, `.md`) for future model-training data.

---

## âš™ï¸ Prerequisites (from Day 1)

* Working Python/VS Code/Jupyter environment
* `requirements.txt` installed
* Git repo initialized and synced

---

## ğŸ›  Agenda (45 min)

|  Time | Segment                                           |
| :---: | :------------------------------------------------ |
|  0â€“5  | Review RTF + PICO concepts                        |
|  5â€“15 | Draft your prompt template                        |
| 15â€“25 | Run and capture outputs (ChatGPT-5 vs Perplexity) |
| 25â€“35 | Evaluate differences + refine                     |
| 35â€“45 | Save, log, commit                                 |

---

## ğŸ§  RTF Formula

**Role â†’ Task â†’ Format**

Example for technical AI prompting:

```
Role: You are an ML engineer designing a regression model for livestock feed efficiency.

Task: Suggest 3 feature-engineering techniques to improve prediction accuracy using real-world agronomic data.  
Include rationale and potential Python libraries.

Format: Return a Markdown table with columns: Technique | Why it helps | Library | Example function
```

---

## ğŸ§­ PICO Framework

| Element      | Purpose                                                                      |
| ------------ | ---------------------------------------------------------------------------- |
| Persona      | Defines authority and tone (e.g., â€œSenior Data Scientist at a research labâ€) |
| Instructions | Explicit steps, criteria, and limitations                                    |
| Context      | Domain, dataset, and environment details                                     |
| Output       | Schema or format to verify against (e.g., JSON, table, markdown)             |

Example:

```
Persona: Senior data scientist working with Microsoft Azure ML.  
Instructions: Evaluate preprocessing methods for predicting cattle feed conversion ratio using probiotics dataset.  
Context: You have structured numeric + categorical features; output needs to be interpretable and reproducible.  
Output: 1) 150-word executive summary; 2) Markdown table (Method | Rationale | Complexity | Expected lift %).
```

---

## ğŸ§® Prompt Testing Steps

1. Choose a topic (e.g., â€œFeed-to-Yield Efficiency AI Model Designâ€).
2. Draft one RTF and one PICO prompt.
3. Run each prompt in ChatGPT-5 and Perplexity.
4. Save outputs as:

   * `Day2_RTF_output.md`
   * `Day2_PICO_output.md`
5. Score each output (1â€“5) on:

   * Structure matches contract
   * Reasoning clarity
   * Data or reference accuracy
   * Consistency (re-run same prompt)
   * Readability for exec audience
6. Record findings in `logs/day2.md`.

---

## ğŸ’¾ Deliverables

* `Day2_structured_prompt.txt` â†’ Finalized prompt templates
* `Day2_prompt_comparison.md` â†’ Side-by-side ChatGPT-5 vs Perplexity outputs + comments
* `logs/day2.md` â†’ Reflection + scorecard
* **Commit:** `feat: Day 2 structured prompt engineering (RTF/PICO)`

---

## âœ… Rubric (Self-Check)

| Criterion                             | Pass | Partial | Fail |
| :------------------------------------ | :--: | :-----: | :--: |
| RTF and PICO templates defined        |   âœ…  |    âš ï¸   |   âŒ  |
| Prompt tested in both tools           |   âœ…  |    âš ï¸   |   âŒ  |
| Outputs saved + comparison documented |   âœ…  |    âš ï¸   |   âŒ  |
| Reflection log completed              |   âœ…  |    âš ï¸   |   âŒ  |

---

## ğŸ§¾ Reflection Prompts

1. Which tool handled structure and schema best?
2. What did ChatGPT-5 improve over 3.5 (reasoning, formatting, citations)?
3. How could this prompt be repurposed for data labeling or evaluation datasets?
4. Whatâ€™s one assumption youâ€™d change to improve transferability to a different domain?

---

## ğŸ“ˆ For Your Mentee (Project Extension)

On Day 3, this prompt becomes a **synthetic data generator**â€”youâ€™ll feed the structured prompts into a Python script that writes JSONL training examples for LLM fine-tuning.

---

